import numpy as np
from nltk import TweetTokenizer
from nltk.classify import MaxentClassifier
from nltk.classify import megam
from megam import config_megam
from nltk.sentiment import SentimentAnalyzer
from nltk.sentiment.util import *
from sklearn.cross_validation import StratifiedKFold
from nltk import compat
from nltk.internals import find_binary
try:
    import numpy
except ImportError:
    numpy = None

from sentiment_util import remove_stopwords, load_datasets
'''configuration megaM'''
_megam_bin = None
def config_megam(bin=None):
    """Configure NLTK's interface to the ``megam`` maxent optimization
    package.

    :param bin: The full path to the ``megam`` binary.  If not specified,
        then nltk will search the system for a ``megam`` binary; and if
        one is not found, it will raise a ``LookupError`` exception.
    :type bin: str
    """
    global _megam_bin
    _megam_bin = find_binary(
        'megam', bin,
        env_vars=['MEGAM'],
        binary_names=['megam.opt', 'megam', 'megam_686', 'megam_i686.opt'],
        url='http://www.umiacs.umd.edu/~hal/megam/index.html')

'''megaM interface functions '''
def write_megam_file(train_toks, encoding, stream, bernoulli=True, explicit=True):

    '''Generate an input file for ``megam`` based on the given corpus of
    classified tokens.'''
    # Look up the set of labels.
    labels = encoding.labels()
    labelnum = dict((label, i) for (i, label) in enumerate(labels))

    # Write the file, which contains one line per instance.
    for featureset, label in train_toks:
        # First, the instance number (or, in the weighted multiclass case, the cost of each label).
        if hasattr(encoding, 'cost'):
            stream.write(':'.join(str(encoding.cost(featureset, label, l))
                                  for l in labels))
        else:
            stream.write('%d' % labelnum[label])

        # For implicit file formats, just list the features that fire
        # for this instance's actual label.
        if not explicit:
            _write_megam_features(encoding.encode(featureset, label),
                                  stream, bernoulli)

        # For explicit formats, list the features that would fire for
        # any of the possible labels.
        else:
            for l in labels:
                stream.write(' #')
                _write_megam_features(encoding.encode(featureset, l),
                                      stream, bernoulli)

        # End of the instance.
        stream.write('\n')

"""Given the stdout output generated by ``megam`` when training a
    model, return a ``numpy`` array containing the corresponding weight
    vector.  This function does not currently handle bias features."""
def parse_megam_weights(s, features_count, explicit=True):
    if numpy is None:
        raise ValueError('This function requires that numpy be installed')
    assert explicit, 'non-explicit not supported yet'
    lines = s.strip().split('\n')
    weights = numpy.zeros(features_count, 'd')
    for line in lines:
        if line.strip():
            fid, weight = line.split()
            weights[int(fid)] = float(weight)
    return weights

def _write_megam_features(vector, stream, bernoulli):
    if not vector:
        raise ValueError('MEGAM classifier requires the use of an always-on feature.')
    for (fid, fval) in vector:
        if bernoulli:
            if fval == 1:
                stream.write(' %s' % fid)
            elif fval != 0:
                raise ValueError('If bernoulli=True, then all'
                                 'features must be binary.')
        else:
            stream.write(' %s %s' % (fid, fval))

"""Call the ``megam`` binary with the given arguments. """
def call_megam(args):
    if isinstance(args, compat.string_types):
        raise TypeError('args should be a list of strings')
    if _megam_bin is None:
        config_megam()

    # Call megam via a subprocess
    cmd = [_megam_bin] + args
    p = subprocess.Popen(cmd, stdout=subprocess.PIPE)
    (stdout, stderr) = p.communicate()

    # Check the return code.
    if p.returncode != 0:
        print()
        print(stderr)
        raise OSError('megam command failed!')

    if isinstance(stdout, compat.string_types):
        return stdout
    else:
        return stdout.decode('utf-8')

def main():
    x, y = load_datasets(["../datasets/sentiment_uci/yelp_labelled.txt"])

    stopwords = set()
    with open('../stopwords.txt', 'r') as f:
        for w in f:
            stopwords.add(w.strip())

    tok = TweetTokenizer()

    x = [remove_stopwords(tok.tokenize(s.lower()), stopwords) for s in x]
    x = np.array(x)

    accumulate = dict()
    folds = 10
    for train_idx, test_idx in StratifiedKFold(y=y, n_folds=folds, shuffle=True):
        train_x, train_y = x[train_idx], y[train_idx]
        test_x, test_y = x[test_idx], y[test_idx]

        write_megam_file(x, encoding, stream, bernoulli=True, explicit=True)

        # train_x = [remove_stopwords(tok.tokenize(s), stopwords) for s in train_x]
        # test_x = [remove_stopwords(tok.tokenize(s), stopwords) for s in test_x]

        train_docs = [(sent, label) for sent, label in zip(train_x, train_y)]
        test_docs = [(sent, label) for sent, label in zip(test_x, test_y)]

        cls = SentimentAnalyzer()

        # train
        words_with_neg = cls.all_words([mark_negation(a) for a in train_x])
        unigram_feats = cls.unigram_word_feats(words_with_neg)

        cls.add_feat_extractor(extract_unigram_feats, unigrams=unigram_feats, handle_negation=True)

        training_set = cls.apply_features(train_docs, labeled=True)

        cls.train(megam.train, training_set, max_iter=10, trace=0)

        # test & evaluate
        test_set = cls.apply_features(test_docs)

        for key, value in sorted(cls.evaluate(test_set).items()):
            print('\t{0}: {1}'.format(key, value))
            accumulate.setdefault(key, 0.0)
            accumulate[key] += value if value is not None else 0.0

    print("Averages")
    for key, value in sorted(accumulate.items()):
        print('\tAverage {0}: {1}'.format(key, value/folds))

if __name__ == '__main__':
    main()